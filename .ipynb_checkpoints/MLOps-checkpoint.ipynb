{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f4d23f",
   "metadata": {},
   "source": [
    "### Introduction to MLOps\n",
    "* Overview\n",
    "  + rule of 25\n",
    "    + 25% effort goes to DevOps\n",
    "    + 25% effort goes to data operations to automate data\n",
    "    + 25% effort goes to modeling and improving modeling\n",
    "    + 25% effort goes to framing or looking at business requirements\n",
    "  + Trends and techniques\n",
    "    + production first mindset\n",
    "      + everything related to the product system needs to be automated\n",
    "* what is devops: combination of \n",
    "  + checklist of software engineering best practice\n",
    "  + culture of continuous improvement\n",
    "  + CI/CD, infrastructure as a code for automation\n",
    "* DataOps\n",
    "  + apply the same principles of devops to data and data systems\n",
    "  + constantly make improvements (data more cleaner)\n",
    "  \n",
    "* cloud programming on github codebase environment\n",
    "  + set up virtual environment by \n",
    "  `virtualenv ~/.venv\n",
    "    source ~/.venv/bin/activate`\n",
    "  + set up the reference to the virtual environment by`vim ~/.bashrc`\n",
    "    + shift g to go to the end of the file, and add `source ~/.venv/bin/activate`\n",
    "  + create Makefile by `touch Makefile`\n",
    "  + create requirments.txt by `touch requirements.txt`\n",
    "  + create a python file such as hello.py and its test file `test_hello.py`\n",
    "  + scafold of Makefile\n",
    "  ```Shell\n",
    "  install:\n",
    "    pip install --upgrade pip &&\\\n",
    "        pip install -r requirements.txt\n",
    "        \n",
    "  test:\n",
    "    python -m pytest -vv test_hello.py\n",
    "    \n",
    "  format:\n",
    "    black *.py\n",
    "    \n",
    "  lint:\n",
    "    pylint --disable=R, C hello.py\n",
    "    \n",
    "  all: install lint test format\n",
    "  ```\n",
    " + requirements.txt\n",
    "  `pytest\n",
    "   pytest-cov\n",
    "   pylint\n",
    "   black\n",
    "   ipython`\n",
    "   \n",
    " + using ipython as a debug tool for your code  \n",
    "   + activate ipython by typing ipython in terminal\n",
    "   + import the function you want to check by `from hello import more_hello` where hello is the .py file and `more_hello` is the function in the file\n",
    "     + you can use pytest grammar such as `assert \"h1\" == more_hello()` "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e1738bd",
   "metadata": {},
   "source": [
    "# template for github actions\n",
    "# in actions part, select \"set up your own workflow\", and use the template\n",
    "# give the file a name. Click \"start commit\" to run the pipeline\n",
    "name: Python application test with Github Actions\n",
    "\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python 3.8\n",
    "      uses: actions/setup-python@v1\n",
    "      with:\n",
    "        python-version: 3.8\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        make install\n",
    "    - name: Lint with pylint\n",
    "      run: |\n",
    "        make lint\n",
    "    - name: Test with pytest\n",
    "      run: |\n",
    "        make test\n",
    "    - name: Format code\n",
    "      run: |\n",
    "        make format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274510f4",
   "metadata": {},
   "source": [
    "### Continuous Delivery (CD)\n",
    "* use a cloud native environment can provide a similar environment to your prod system\n",
    "* you push the code and Infrastructure as code (IAC) to the cloud environment to provision the resources and change code\n",
    "* source control repo will trigger build system component to test the code and merge code\n",
    "* build system will then create/mutate/delete infrastructure (IAC) will make sure the states of infrastructure is idempotent\n",
    "\n",
    "### Feature Store\n",
    "* data warehouse vs feature store\n",
    "  + data lake contains raw data including a variety of source data, such as streaming data, batch data, structured and unstructured data\n",
    "  + one way to use the data is the featurization that transform the data to reusable, high quality ML inputs in feature store\n",
    "    + the high quality data in feature store can be used to train models and make predictions, being audited and extract relationships between features\n",
    "  + another way is to use ETL to transform data data in data warehouse as reusable, high-quality BI inputs for building reporting, dashboards and BI systems\n",
    "  \n",
    "### Data Drift\n",
    "* when data drifts constantly, it is difficult to build an efficient model. Therefore, retraining the model is usually necessary\n",
    "\n",
    "### Operationalizing a microservice\n",
    "* ![image.png](attachment:image.png)\n",
    "* microservice style: \n",
    "  + git code can be used as\n",
    "    + small reusable code\n",
    "    + CLI\n",
    "    + library\n",
    "    + microservice service\n",
    "    + container\n",
    "  + we can build microservice in stage environment using CD build server to execute lint test, compile, and deploy (IAC)\n",
    "    + stage environment can collect health metrics such as CPU, memory and latency by metrics and monitoring system\n",
    "    + it can also have performance verification for loadtesting\n",
    "  + if the stage environment test is passed, we can easily create \"N\" microservice environments in prod environment\n",
    "  \n",
    "### CI for microservices\n",
    "* local prod environment (manually)\n",
    "  + set up test, lint and format processes by makefile and use build server to automate the operations\n",
    "  \n",
    "### End to End MLOps HuggingFace Spaces\n",
    "* ![image-2.png](attachment:image-2.png)\n",
    "* in hugging face, create a authentication token to be used in github codespace and github actions\n",
    "* github pull hugging face models from hugging Face, and goes to hugging Face spaces and uses Gradio App for text summarization\n",
    "\n",
    "#### Hugging face\n",
    "* in hugging face, models contain a lot of pre-trained models\n",
    "* in spaces, you can deploy your apps\n",
    "* first, create a space, call it demo2, and select Gradio as the SDK\n",
    "* after the space is created, copy the git clone command\n",
    "* go to github repo, and create a new repo and name it as hugging-face-demo2. Add a readme file and .gitignore template for python\n",
    "* in github -> code, select \"Configure and create codespace\", which allows you to select machine tyhpe with more memory and storage\n",
    "* create a scafold of the project, including\n",
    "  + a virtual environment (create by `virtualenv ~/.venv`)\n",
    "  + complile ~/.bashrc by `vim ~/.bashrc` and add `source ~/.venv/bin/activate` at the end of the file. This will load the virtual environment every time you open a shell\n",
    "  + create requirements.txt, app.py and Makefile by touch from terminal\n",
    "  + copy and paste Makefile template to Makefile\n",
    "  + edit the requirements.txt for packages\n",
    "    + gradio, transformers, and tensorflow\n",
    "  + in app.py add\n",
    "  ```python\n",
    "    from transformers import pipeline\n",
    "    import gradio as gr\n",
    "    \n",
    "    model = pipeline(\"summarization\")\n",
    "    \n",
    "    def predict(prompt):\n",
    "        summary = model(prompt)[0][\"Summary_text\"]\n",
    "        return summary\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        textbox = gr.Textbox(placeholder=\"Enter text block to summarize\", lines=4)\n",
    "        gr.Interface(fn=predict, inputs=textbox, outputs=\"text\")\n",
    "        \n",
    "    demo.launch()\n",
    "    \n",
    "  ```\n",
    "  + type `make install` to install packages\n",
    "  \n",
    "* set up hugging face\n",
    "  + in hugging face, select your profile, and settings\n",
    "  + create a new access token in \"Access Token\" section. Select the role of write, and name it github-actions-deploy\n",
    "  + copy the token, and go to the repo, and go to settings, got to Secrets/actions, and create new repo secret. Paste the value there and name it HG\n",
    "  \n",
    "* set up workflow in git repo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a26e10c",
   "metadata": {},
   "source": [
    "name: Sync to Hugging Face hub\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "\n",
    "  # to run this workflow manually from the Actions tab\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  sync-to-hub:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "        with:\n",
    "          fetch-depth: 0\n",
    "      - name: Add remote \n",
    "        env:\n",
    "          HF: ${{ secrets.HF }}\n",
    "        run: git remote add space https://noahgift:$HF@huggingface.co/spaces/noahgift/demo\n",
    "      - name: Push to hub\n",
    "        env:\n",
    "          HF: ${{ secrets.HF }}\n",
    "        run: git push --force https://noahgift:$HF@huggingface.co/spaces/noahgift/demo main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3dc432",
   "metadata": {},
   "source": [
    "### fastAPI\n",
    "* in AWS cloud9 / app runner\n",
    "* noahgift/fastapi (https://github.com/noahgift/fastapi)\n",
    "\n",
    "### Makefile\n",
    "* check the version of make version by `which make`. This works on both Linux and windows\n",
    "* create Makefile using touch Makefile\n",
    "* content of the Makefile is the shell commands with sections defined by string and :\n",
    "  ```\n",
    "    hello\n",
    "        echo \"this is my first make command\"\n",
    "  ```    \n",
    "\n",
    "### Three most important files in a python project\n",
    "* Makefile: automate pipelines as a recipe\n",
    "* Dockerfile: provides runtime\n",
    "* requirements.txt: for python package requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ee2f7",
   "metadata": {},
   "source": [
    "## Data science\n",
    "\n",
    "### Data science structure\n",
    "* Ingest\n",
    "* EDA\n",
    "* modeling\n",
    "* conclusion\n",
    "\n",
    "### A typical process\n",
    "* start a new github repo\n",
    "* select README and .gitignore options and create the repo\n",
    "* edit README file using markdown\n",
    "* to run the code, you can use either github codespace or google colab. Colab can be connected to github\n",
    "* a common workflow is to run Makefile for install, test and deploy, and then use GitHub Actions to push code to Amazon ECR, then use App Runner to deploy the code to FastAPI\n",
    "\n",
    "### test notebook\n",
    "* how to hook colab notebook with github\n",
    "* in github, set up Makefile to test notebook using `python -m pytest --nbval notebook.ipynb`\n",
    "* convert the Makefile to action workflow of github. Each time when you push a change to your notebook, the build process will automatically run and validate your notebook\n",
    "\n",
    "### Github workspace template\n",
    "* microsoft / codespaces-teaching-template-py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
